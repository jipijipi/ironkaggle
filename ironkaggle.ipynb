{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# IRON KAGGLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **shop_ID**: Unique identifier for each shop.\n",
    "- **day_of_the_week**: Encoded from 1 to 7, representing the day of the week.\n",
    "- **date**: Day, month, and year of the data point.\n",
    "- **number_of_customers**: Quantity of customers that visited the shop on that day.\n",
    "- **open**: Binary variable; 0 means the shop was closed, while 1 means it was open.\n",
    "- **promotion**: Binary variable; 0 means no promotions, 1 means there were promotions.\n",
    "- **state_holiday**: Encoded as 0, 'a', 'b', 'c', indicating the presence of a state holiday (0 if none). 'a', 'b', 'c' represent different state holidays.\n",
    "- **school_holiday**: Binary variable; 0 means no school holiday, 1 means there was a school holiday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lib Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, root_mean_squared_error\n",
    "\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_store_stats(df):\n",
    "\n",
    "    store_stats = df.groupby('Store_ID').agg({\n",
    "        'Sales': ['mean', 'std', 'median'],\n",
    "        'Nb_customers_on_day': ['mean', 'std', 'median'],\n",
    "        'Promotion': 'mean',  # Promotion frequency\n",
    "        'School_holiday': 'mean',  # School holiday frequency\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten column names\n",
    "    store_stats.columns = ['Store_ID'] + [\n",
    "        f'Store_{col[0]}_{col[1]}' for col in store_stats.columns[1:]\n",
    "    ]\n",
    "\n",
    "    # Replace infinity values with NaN\n",
    "    store_stats.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Calculate store performance quartiles\n",
    "    store_stats['Store_sales_quartile'] = pd.qcut(\n",
    "        store_stats['Store_Sales_mean'],\n",
    "        q=4,\n",
    "        labels=['Q1', 'Q2', 'Q3', 'Q4']\n",
    "    )\n",
    "\n",
    "    return store_stats\n",
    "\n",
    "def transform_data(df, store_stats=None):\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Remove rows where 'Open' == 0\n",
    "    #if 'Open' in df.columns:\n",
    "     #   df = df[df['Open'] != 0]\n",
    "\n",
    "    # Convert 'Date' to datetime\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    # Extract date features\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Day'] = df['Date'].dt.day\n",
    "\n",
    "    # Create 'Is_weekend' feature\n",
    "    if 'Day_of_week' in df.columns:\n",
    "        df['Is_weekend'] = df['Day_of_week'].apply(lambda x: 1 if x >= 6 else 0)\n",
    "    else:\n",
    "        df['Is_weekend'] = np.nan\n",
    "\n",
    "    # Create 'Sales_per_customer' if possible\n",
    "    if 'Sales' in df.columns and 'Nb_customers_on_day' in df.columns:\n",
    "        df['Sales_per_customer'] = df['Sales'] / df['Nb_customers_on_day']\n",
    "        # Replace infinity values with NaN\n",
    "        df['Sales_per_customer'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    else:\n",
    "        df['Sales_per_customer'] = np.nan\n",
    "\n",
    "    # Create 'Promo_School_Holiday'\n",
    "    if 'Promotion' in df.columns and 'School_holiday' in df.columns:\n",
    "        df['Promo_School_Holiday'] = df['Promotion'] * df['School_holiday']\n",
    "    else:\n",
    "        df['Promo_School_Holiday'] = np.nan\n",
    "\n",
    "    # Merge store_stats if available\n",
    "    if store_stats is not None:\n",
    "        df = df.merge(store_stats, on='Store_ID', how='left')\n",
    "\n",
    "    # One-hot encode 'State_holiday' and 'Store_sales_quartile'\n",
    "    columns_to_encode = []\n",
    "    if 'State_holiday' in df.columns:\n",
    "        columns_to_encode.append('State_holiday')\n",
    "    if 'Store_sales_quartile' in df.columns:\n",
    "        columns_to_encode.append('Store_sales_quartile')\n",
    "    if columns_to_encode:\n",
    "        df = pd.get_dummies(df, columns=columns_to_encode, drop_first=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/sales.csv')\n",
    "\n",
    "test_data_no_target = pd.read_csv('data/ironkaggle_notarget.csv')\n",
    "solutions = pd.read_csv('data/ironkaggle_solutions.csv')\n",
    "test_data = test_data_no_target.merge(solutions, on='True_index')\n",
    "\n",
    "sales_df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute store statistics\n",
    "store_stats = compute_store_stats(sales_df)\n",
    "\n",
    "# Transform the sales data\n",
    "transformed_sales_data = transform_data(sales_df, store_stats=store_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute store statistics\n",
    "store_stats_test = compute_store_stats(test_data)\n",
    "\n",
    "# Transform the test data\n",
    "transformed_test_data = transform_data(test_data, store_stats=store_stats_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define feature & target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = transformed_sales_data.copy()\n",
    "test_data = transformed_test_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sales_df)\n",
    "display(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = sales_df.drop(columns=['Sales','Date', 'True_index','Store_ID','Open'])\n",
    "target = sales_df['Sales']\n",
    "\n",
    "test_target = test_data[['Sales']]\n",
    "test_data = test_data.drop(columns=['Sales','Date', 'True_index','Store_ID','Open'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Try different sample sizes\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Try different scaler\n",
    "normalizer = MinMaxScaler()\n",
    "\n",
    "normalizer.fit(X_train)\n",
    "\n",
    "X_train_norm = normalizer.transform(X_train)\n",
    "X_test_norm = normalizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_normalizer = MinMaxScaler()\n",
    "\n",
    "test_normalizer.fit(test_data)\n",
    "\n",
    "test_norm = normalizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 3\n",
    "max_depth = None\n",
    "n_estimators = None\n",
    "max_samples = None\n",
    "max_leafs_nodes = None\n",
    "max_features = None\n",
    "\n",
    "model = KNeighborsRegressor(n_neighbors=n_neighbors, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 10\n",
    "max_depth = None\n",
    "n_estimators = None\n",
    "max_samples = None\n",
    "max_leafs_nodes = None\n",
    "max_features = None\n",
    "\n",
    "model = KNeighborsRegressor(n_neighbors=n_neighbors, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = None\n",
    "max_depth = None\n",
    "n_estimators = None\n",
    "max_samples = None\n",
    "max_leafs_nodes = None\n",
    "max_features = None\n",
    "\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = None\n",
    "max_depth = 10\n",
    "n_estimators = None\n",
    "max_samples = None\n",
    "max_leafs_nodes = None\n",
    "max_features = None\n",
    "\n",
    "model = DecisionTreeRegressor(max_depth=max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = None\n",
    "max_depth = 1000\n",
    "n_estimators = None\n",
    "max_samples = None\n",
    "max_leafs_nodes = None\n",
    "max_features = None\n",
    "\n",
    "model = DecisionTreeRegressor(max_depth=max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = None\n",
    "max_depth = 1000\n",
    "n_estimators = None\n",
    "max_samples = None\n",
    "max_leafs_nodes = None\n",
    "max_features = None\n",
    "\n",
    "model = DecisionTreeRegressor(max_depth=max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = None\n",
    "max_depth = None\n",
    "n_estimators = 100\n",
    "max_samples = 1000\n",
    "max_leafs_nodes = None\n",
    "max_features = None\n",
    "estimator = LinearRegression()\n",
    "\n",
    "model = BaggingRegressor(estimator, n_estimators=n_estimators, max_samples=max_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 10\n",
    "max_depth = None\n",
    "n_estimators = 100\n",
    "max_samples = 1000\n",
    "max_leafs_nodes = None\n",
    "max_features = None\n",
    "estimator = KNeighborsRegressor(n_neighbors=n_neighbors, n_jobs=-1)\n",
    "\n",
    "model = BaggingRegressor(estimator, n_estimators=n_estimators, max_samples=max_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = None\n",
    "max_depth = 1000\n",
    "n_estimators = 100\n",
    "max_samples = 1000\n",
    "max_leafs_nodes = None\n",
    "max_features = None\n",
    "estimator = DecisionTreeRegressor(max_depth=max_depth)\n",
    "\n",
    "model = BaggingRegressor(estimator, n_estimators=n_estimators, max_samples=max_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = None\n",
    "max_depth = 1000\n",
    "n_estimators = 100\n",
    "max_samples = 1000\n",
    "max_leafs_nodes = None\n",
    "max_features = None\n",
    "estimator = DecisionTreeRegressor(max_depth=max_depth)\n",
    "\n",
    "model = BaggingRegressor(estimator, n_estimators=n_estimators, max_samples=max_samples, bootstrap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = None\n",
    "max_depth = 1000\n",
    "n_estimators = 100\n",
    "max_samples = None\n",
    "max_leafs_nodes = None\n",
    "max_features = None\n",
    "estimator = None\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = None\n",
    "max_depth = 100\n",
    "n_estimators = 100\n",
    "max_samples = None\n",
    "max_leafs_nodes = None\n",
    "max_features = None\n",
    "estimator = DecisionTreeRegressor(max_depth=max_depth)\n",
    "\n",
    "model = AdaBoostRegressor(n_estimators=n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 10\n",
    "max_depth = None\n",
    "n_estimators = 100\n",
    "max_samples = None\n",
    "max_leafs_nodes = None\n",
    "max_features = None\n",
    "estimator = KNeighborsRegressor(n_neighbors=n_neighbors, n_jobs=-1)\n",
    "\n",
    "model = AdaBoostRegressor(n_estimators=n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = None\n",
    "max_depth = None\n",
    "n_estimators = 100\n",
    "max_samples = None\n",
    "max_leafs_nodes = None\n",
    "max_features = None\n",
    "estimator = LinearRegression()\n",
    "\n",
    "model = AdaBoostRegressor(n_estimators=n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = None\n",
    "max_depth = 100\n",
    "n_estimators = 100\n",
    "max_samples = None\n",
    "max_leafs_nodes = None\n",
    "max_features = None\n",
    "estimator = None\n",
    "\n",
    "model = GradientBoostingRegressor(max_depth=max_depth,  n_estimators=n_estimators, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = None\n",
    "max_depth = 20\n",
    "n_estimators = 100\n",
    "max_samples = None\n",
    "max_leafs_nodes = None\n",
    "max_features = None\n",
    "estimator = None\n",
    "\n",
    "model = GradientBoostingRegressor(max_depth=max_depth,  n_estimators=n_estimators, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = None\n",
    "max_depth = 400\n",
    "n_estimators = 50\n",
    "max_samples = None\n",
    "max_leafs_nodes = None\n",
    "max_features = None\n",
    "estimator = None\n",
    "\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=n_estimators, max_depth=max_depth, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 200],\n",
    "    'max_depth': [10, 400],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = GridSearchCV(estimator = RandomForestRegressor(), param_grid = param_grid, cv=5, n_jobs=-1, verbose=4) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_norm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model = model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Convert the cv_results_ to a DataFrame\n",
    "results_df = pd.DataFrame(model.cv_results_)\n",
    "\n",
    "# Define the file name\n",
    "file_name = 'grid_search_results.csv'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.isfile(file_name):\n",
    "    # If the file exists, append the new results without writing the header\n",
    "    results_df.to_csv(file_name, mode='a', header=False, index=False)\n",
    "else:\n",
    "    # If the file does not exist, create it and write the header\n",
    "    results_df.to_csv(file_name, mode='w', header=True, index=False) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the regressor name and the estimator name programmatically\n",
    "estimator_name = type(estimator).__name__\n",
    "model_name = type(model).__name__\n",
    "\n",
    "pred = model.predict(test_norm)\n",
    "\n",
    "test_data_no_target['Predicted_Sales'] = pred\n",
    "result = test_data_no_target[['True_index', 'Predicted_Sales']]\n",
    "\n",
    "mae = mean_absolute_error(pred, test_target)\n",
    "rmse = root_mean_squared_error(pred, test_target)\n",
    "r2 = model.score(test_norm, test_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the current model results\n",
    "current_results = pd.DataFrame([{\n",
    "    'Model': f'{model_name} > {estimator_name}' if 'estimator_name' in locals() else model_name,\n",
    "    'n_neighbors': n_neighbors,\n",
    "    'max_depth': max_depth,\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_samples': max_samples,\n",
    "    'max_leafs_nodes': max_leafs_nodes,\n",
    "    'max_features' : max_features,\n",
    "    'MAE': mae,\n",
    "    'RMSE': rmse,\n",
    "    'R2_score': r2,\n",
    "}])\n",
    "\n",
    "display(current_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name = 'results.csv'\n",
    "# Check if the file already exists\n",
    "if os.path.isfile(file_name):\n",
    "    # If the file exists, append the new results without writing the header\n",
    "    current_results.to_csv(file_name, mode='a', header=False, index=False)\n",
    "else:\n",
    "    # If the file does not exist, create it and write the header\n",
    "    current_results.to_csv(file_name, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('ironkaggle_final_batman_robin.csv', mode='a', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
